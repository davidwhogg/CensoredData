% Define the document class. aastex for manuscript preparation, emulateapj for making it look polished.
% \documentclass[12pt,preprint2]{aastex}
%\documentclass[12pt,a4paper, iop,onecolumn,numberedappendix]{emulateapj}
\documentclass[12pt,preprint]{aastex}

% Import the natbib package for bibliography inclusion. To get it to work right, first typeset the bibliography (BibTex), Command-Shift-B.
% Then, typeset the document with LaTeX, Command-Shift-L or just Command-T.
% To get proper reference citations using aastex (preprint manuscripts), first typeset as emulateapj with BibTex to create the BibTex 
% files and then typeset again as aastex to get the manuscript with references.
\usepackage{natbib}
\usepackage{epsfig,graphicx,color}
\usepackage{longtable}

\bibliographystyle{apj}
% A few colors to replace the defaults for certain link types
\definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
\definecolor{darkorange}{rgb}{.71,0.21,0.01}
\definecolor{darkgreen}{rgb}{.12,.54,.11}

%-----------------------------------------------------------------------------
% The hyperref package gives us a pdf with properly built
% internal navigation ('pdf bookmarks' for the table of contents,
% internal cross-reference links, web links for URLs, etc.)
\usepackage{hyperref}
\hypersetup{pdftex,  % needed for pdflatex
  breaklinks=true,  % so long urls are correctly broken across lines
  colorlinks=true,
  urlcolor=blue,
  linkcolor=darkorange,
  citecolor=darkgreen,
  }

%Define this shorthand for scientific notation.
\newcommand{\expnt}[2]{\ensuremath{#1 \times 10^{#2}}}   % scientific notation
\newcommand{\x}{{\bf x}}
\newcommand{\X}{{\bf X}}
\newcommand{\y}{{\bf y}}
\newcommand{\z}{{\bf z}}
\newcommand{\new}{red}
\newcommand{\ptr}{p_{\rm tr}}
\newcommand{\pte}{p_{\rm te}}
\newcommand{\Ex}{\mathbf{E}}
\newcommand{\Prob}{\mathbf{P}}
\newcommand{\PrfL}{\widehat{P}_{\rm{RF,}\mathcal{L}}(y|\x)}
\newcommand{\PrfLx}{\widehat{P}_{\rm{RF,}\mathcal{L}\cup\x'}(y|\x)}
\newcommand{\TreeL}{\theta_{b, \mathcal{L}}(y|\x)}
\newcommand{\TreeLx}{\theta_{b, \mathcal{L}\cup\x'}(y|\x)}
 \def\hipp {{\it Hipparcos~}}

\newcommand{\dd}{\mathrm{d}}

\newcommand{\fobs}{f_i^*}
\newcommand{\sobs}{s_i^*}

% Commands for annotating the docs with fixme and inter-author notes.  See
% below for how to disable these.
%
% Define a \fixme command to mark visually things needing fixing in the draft,
% as well as similar commands for each author to leave initialed special
% comments in the document.
% For final printing or to simply disable these bright warnings, symlink
% (there's a target 'dist_on' in the makefile that does this) the file
% macros_state.tex to macros_off.tex

\newcommand{\fixme}[1] { \textcolor{red} {
{\fbox{ {\bf FIX}
\ensuremath{\blacktriangleright \blacktriangleright \blacktriangleright}}
{\bf #1}
\fbox{\ensuremath{\blacktriangleleft \blacktriangleleft \blacktriangleleft}
} } } }

% And similarly, one (less jarring, with fewer symbols and no boldface) command
% for each one of us to leave comments in the main text.
\newcommand{\james}[1] { \textcolor{blue} {
\ensuremath{\blacklozenge} {\bf james:}  {#1}
\ensuremath{\blacklozenge} } }

\newcommand{\hogg}[1] { \textcolor{darkorange} {
\ensuremath{\blacksquare} {\bf hogg:}  {#1}
\ensuremath{\blacksquare} } }

\newcommand{\dan}[1] { \textcolor{darkgreen} {
\ensuremath{\bigstar} {\bf dan:}  {#1}
\ensuremath{\bigstar} } }

\newcommand{\joey}[1] { \textcolor{red} {
\ensuremath{\clubsuit} {\bf joey:}  {#1}
\ensuremath{\clubsuit} } }%\documentclass[manuscript]{aastex}


\begin{document}
%%%%%%%% TITLE, AUTHORS, PUBLICATION STATUS, and ABSTRACT %%%%%%%%%

\shorttitle{Fitting the undetected}
\shortauthors{}
\title{Fitting the undetected:  Light-curve feature estimation in the presence of carelessly reported non-detections}
\author{
TBD
}
%
%\altaffiltext{1}{Astronomy Department, University of California, Berkeley, CA, 94720-7450, USA}
%\altaffiltext{2}{Statistics Department, University of California, Berkeley, CA, 94720-7450, USA}
%\altaffiltext{*}{E-mail: {\tt jwrichar@stat.berkeley.edu}}

% Note the status (in preparation, MNRAS submitted date, ApJ accepted date, in press etc).
\slugcomment{in preparation}


\begin{abstract}
In multi-epoch imaging surveys, faint variable sources will not be
detectable at all epochs.  In principle, the best approach is to
perform photometric measurements anyway, so that there is a datum
required at at every epoch.  In practice, in most surveys we are
provided at some epochs only with upper limits; in some not even upper
limits but just the information that the star was not observed.  Here
we demonstrate with real data on periodic variables from the All-Sky
Automated Survey that these non-detections are nonetheless useful in
fitting and parameter estimation; their inclusion improves both
precision and accuracy.  One novel aspect of this work is that we do
not require the data source to include accurate---or even
any---information about the uncertainty variances on detections or the
values of the upper limits; we find that we can model these as latent
variables, even when they are different at every epoch.
\end{abstract}

%\keywords{stars: variables: general -- methods: data analysis -- methods: statistical -- techniques: photometric}

\section{Introduction}
\label{sec:intro}


For each astronomical object, a photometric survey (think of it as operating in a single photometric bandpass for now) repeatedly scans over the celestial position of a source, recording a multi-epoch light curve over $N$ (typically unevenly spaced) epochs.  At each epoch $i$, with associated time $t_i$, the survey takes an exposure at the location of the object and either (a) detects the object and records an estimate of its photon flux, $\fobs$ and the root variance of the statistical uncertainty in that estimate, $\sobs$, or (b) fails to detect the object.  In the latter case, most modern surveys either record a reference value to signify that no detection was made (as is done in ASAS) or an estimate of the upper detection limit, $b_i$, which is the brightest the object could have been given that it was not detected at a significant level by the software.

There are many reasons that a source might not show up in a catalog.  These include:
\begin{itemize}
\item low S/N of the source, due either to a higher noise level (e.g., clouds) or a fainter signal (e.g. intrinsic dimming of the source),
\item the source falling outside of the detection window (e.g., near chip edge),
\item occulting of the source by an artifact of the detector (e.g., hot pixels, masked out), or
\item the source was out-shone by an intervening object (e.g., asteroid, comet, variable star, airplane, etc.).
\end{itemize}
Here, we present a statistical model that can be used to detect variable sources and model their variability using multi-epoch light curves containing epochs of non-detection.  Previous efforts to detect and model variability using multi-epoch photometry have typically ignored non-detections (REFs) or used them in an ad-hoc manner lacking statistical rigor (REFs).  An exception is \citet{2009AJ....137.4400L}, who measure proper motions of sources in SDSS falling below the detection limit.


%A photometric survey (think of it as operating in a single photometric
%bandpass for now) scans over the celestial position of a particular
%variable star at a large number of times $t_i$.  At each of these
%times, the imaging data are analyzed by (not awesome) software, which
%treats each scan as a \emph{completely independent} survey.  That is,
%when analyzing the data from time $t_i$, none of the information from
%any of the other times is used in any way.
%
%At each time $t_i$, the star is either detected ($q_i=1$) by the
%software or not ($q_i=0$).  When it is detected, the software returns
%a (possibly bad) flux value $f_i$ and (likely bad) uncertainty
%variance $s_i^2$.  When the source is \emph{not} detected ($q_i=0$),
%nothing is reported---in this case, because the software is not
%awesome, it doesn't see any reason to report anything at all, since on
%the non-detect passes, it has no inkling that this patch of the sky is
%interesting in any way.  Welcome to the desert of the real.
%
%Beyond this, there are two additional issues.  The first is that each
%night of imaging is different in an unreported and unknown way.  That
%is, there is different transparency, sky brightness, and point-spread
%function.  So the detection limit, or completeness level, or censoring
%of the catalog is different every night.  The second issue is that
%either because we are getting the data from a non-generous source, or
%because the conditions change rapidly and unpredictably, we can't
%analyze all the sources in a finite patch simultaneously.  For each
%variable star of interest, we \emph{only} get the data on that star
%itself.


\joey{discuss in context of: parameter estimation (amplitude / period), discovery (we're gonna find a lot more variable sources if we take the non-detections into account), and classification; lossiness}

\joey{RRL end at 120 kpc in Sesar et al. paper}

\joey{Numbers of LCs affected: Stripe 82, ASAS}




\section{Method}
\label{sec:method}

\subsection{Preliminaries}
\label{ss:prelim}


For each epoch, $t_i$, the photometric light curve of an astronomical source contains an estimate the mean brightness, $\fobs$, and root-variance of the error in that estimate, $\sobs$.  These quantities are estimated from pixelated images taken of the source.   In this paper, we will assume that $\fobs$ and $\sobs$ take on real-valued numbers in epochs for which the source was detected and receive the reference value {\tt NA} in epochs where no detection was made. For notational convenience we assemble all the light curve data for one source into a data set $D$ given by
\begin{eqnarray}\displaystyle
D &\equiv& \{D_i\}
\\
D_i &\equiv& (\fobs, \sobs)
\quad ,
\end{eqnarray}
where $(\fobs, \sobs)$ are the light curve measurements  at $t_i$.  The goal, for each astronomical object, is to construct the likelihood, $f^n$, for the data $D$, given a set of model parameters that describe the variability of the object and the characteristics of the observations, and either to maximize the likelihood with respect to the model parameters or to use it in further inference.


\subsection{The Light Curve Model}

To model the mean brightness of the light curve as a function of time, we use a multiple-harmonic Fourier model with angular oscillation frequency $\omega$,
\begin{eqnarray}\displaystyle
\mu_i &=& \sum_{k=1}^K A_k\, \sin (t_i \, \omega  k) + B_k\, \cos (t_i \, \omega  k)
\quad ,\label{eq:fourier}
\end{eqnarray}
where $\sqrt{A_k^2 + B_k^2}$ is the amplitude of the $k^{\rm th}$ harmonic of the frequency $\omega$ and $\tan^{-1}(B_k,A_k)$ is the relative phase offset of harmonic $k$.  In this analysis, we will fix the number of harmonics to $K=3$, though in principle we can use the data to choose the appropriate $K$ to capture the complexity of the light curve.  In addition to the expected brightness in Equation \ref{eq:fourier}, we assume that there is uncertainty or inappropriateness in the model, leading to a \emph{model variance} $s_\mu^2$ at each point (assumed constant but easily generalized).


Additionally, we need to instantiate a latent variable, $b_i$, to represent the detection limit, in units of flux, at epoch $i$.  The $b_i$ parameter is essential because it constrains the possible values of the mean brightness, $\mu_i$, of the light curve when there is a non-detection.  By employing a hierarchical model for the distribution of $b_i$, we can fully utilize all of the information encoded in both the detections and non-detections when computing the data likelihood.   At each epoch, we connect the observed flux, $\fobs$, to the latent variable, $f_i$, signifying the true observable flux of the object, via the detection limit, $b_i$ by
\begin{eqnarray}\displaystyle
\fobs &=& \left\{\begin{array}{ll}
  f_i & \mbox{if $f_i \ge b_i$} \\
  \texttt{NA} & \mbox{if $f_i < b_i$} \\
\end{array} \right.
\end{eqnarray}
so that the observed flux is \texttt{NA} only when the true observable flux, $f_i$, is below the detection limit, $b_i$.  And similarly, we introduce a latent variable, $s_i$, for the true observable standard error,
\begin{eqnarray}\displaystyle
\sobs &=& \left\{\begin{array}{ll}
  s_i & \mbox{if $f_i \ge b_i$} \\
  \texttt{NA} & \mbox{if $f_i < b_i$} \\
\end{array} \right.
\end{eqnarray}
so that the true observable standard error of the flux measurement is \texttt{NA} only when the true observable flux is below the detection limit.

Also, because we do not necessarily believe the reported measurement uncertainties, $\sobs$, we choose to introduce a parameter, $\sigma_i$, to represent the true uncertainty variance for the brightness measurement at epoch $i$.

Hence, our initial model consists of the parameter vectors $\{f_1,...,f_N\}$, $\{s_1,...,s_N\}$, $\{b_1,...,b_N\}$ and $\{\sigma_1,...,\sigma_N\}$, and the model parameters
\begin{eqnarray}\displaystyle
\theta &\equiv& (\omega, \{A_k\}, \{B_k\}, s_\mu^2) \quad ,
\end{eqnarray}
along with prior information about observation times, $\{t_1,...,t_N\}$ and other prior assumptions, which we make explicit by creating the prior information set
\begin{eqnarray}\displaystyle
I &\equiv& (\{t_i\}, \mbox{assumptions}) \quad .
\end{eqnarray}
Our goal is to write down the form of the likelihood of the data, $D$, given $\theta$ and $I$.  Then, for each light curve we can find the vector $\theta$ that maximizes the data likelihood.


\subsection{Statistical Model for Light Curves with Non-Detections}

We model the observed flux, $\fobs$, as a Gaussian distribution with variance that has both measurement ($\sigma_i$) and model ($s_{\mu}$) contributions.  In the case that a detection is made, we require that the observed brightness be greater than the brightness of the detection limit ($\fobs = f_i \ge b_i$) while in the case that no detection is made, we require that the brightness (if it could be measured) be less than the detection limit ($f_i < b_i$).  To derive the likelihood of the observed $\fobs$, given $\sigma_i$, $\theta$ and $I$, we must integrate over the prior distribution of the unknown $b_i$,
\begin{eqnarray}\displaystyle
p(\fobs |\sigma_i,\theta,I) &=& \left\{\begin{array}{ll}
  N(f_i | \mu_i,  \sigma_i^2 + s_{\mu}^2)\,  \int_0^{f_i} p(b_i | \theta)\, \dd b_i & \mbox{if $\fobs \ne \texttt{NA}$} \\
  \int_{0}^{\infty} \int_{0}^{b_i} N(f_i | \mu_i, \sigma_i^2 + s_{\mu}^2)\, p(b_i | \theta)\, \dd f_i\, \dd b_i & \mbox{if $\fobs = \texttt{NA}$} \\
\end{array}\right.\label{eq:mlik}
\\
p(b_i|\theta) &=& N(b_i|B,V_B)
\label{eq:bprior}
\\
\theta &\equiv& (\omega, \{A_k\}, \{B_k\}, s_\mu^2, B, V_B) \quad ,
\end{eqnarray}
where we have introduced the hyperparameters $B$ and $V_B$ for the Gaussian prior distribution of $b_i$.  In Equation \ref{eq:mlik}, in the epochs for which a detection was made ($\fobs \ne \texttt{NA}$), we marginalize over the unknown $b_i$ from $0$ (low brightness limit) to $f_i$, enforcing that the detection limit be fainter than the observed brightness.  Likewise, in the epochs for which no detection was made ($\fobs = \texttt{NA}$), we integrate the joint $(f_i, b_i)$ likelihood over all possible values of $b_i$ and over the unknown $f_i$ from $0$ to $b_i$, ensuring that the brightness (if it were able to be observed) be fainter than the detection limit.

In the above, we have assumed no extra information on each of the $b_i$ values besides the knowledge of whether a detection was made on that epoch.  Hence, we draw, in Equation \ref{eq:bprior}, each $b_i$ value from a global prior distribution which is the same at all epochs.  If instead, we are given an estimate of $b_i$ plus its error distribution for each epoch (which, in principle can be inferred from the raw telescope images), we can replace Equation \ref{eq:bprior} with a different distribution per epoch.  In the case that the $b_i$ are assumed to be completely known (without error), the data likelihood of $\fobs$ becomes
\begin{eqnarray}\displaystyle
p(\fobs |\sigma_i,\theta,I, b_i) &=& \left\{\begin{array}{ll}
  N(f_i | \mu_i,  \sigma_i^2 + s_{\mu}^2)\,  I(f_i \ge b_i) & \mbox{if $\fobs \ne \texttt{NA}$} \\
 \int_{0}^{b_i} N(f_i | \mu_i, \sigma_i^2 + s_{\mu}^2)\, \dd f_i & \mbox{if $\fobs = \texttt{NA}$} \\
\end{array}\right.\label{eq:mlik_s}
\end{eqnarray}
where the boolean indictor function, $I(f_i \ge b_i)$, is 1 if $f_i \ge b_i$ and 0 otherwise.

Next, we model the reported standard error, $\sobs$, on the uncertainty of the magnitude measurement.  Instead of assuming that $\sobs$ is a perfect, error-free measurement, we probabilistically connect it to the true uncertainty standard error, $\sigma_i$ through a truncated Gaussian likelihood, where the truncation enforces a non-negative variance for each observation.  Our likelihood of observed $\sobs$, given $\sigma_i$, $\theta$ and $I$, is
\begin{eqnarray}\displaystyle
p(\sobs | \sigma_i, \theta,I) &=& \left\{\begin{array}{ll}
  N(s_i|\sigma_i,v_\sigma) & \mbox{if $\sobs \ne \texttt{NA}$} \\
  \int_{0}^{\infty}  N(s_i | \sigma_i,v_\sigma)\,  \dd s_i & \mbox{if $\sobs = \texttt{NA}$}
\end{array}\right.\label{eq:slik}
\\
\theta &\equiv& (\omega, \{A_k\}, \{B_k\}, s_\mu^2, B, V_B ,v_\sigma) \quad ,
\end{eqnarray}
\joey{Need a distribution that is truncated at 0. Something better? }
where we have added a model parameter $v_\sigma$, which represents the
variance in the distribution of reported uncertainties given the true
uncertainty.  The only purpose of 
 including the likelihood in Equation \ref{eq:slik} to the model, in practice, to keep the $\sigma_i$ from
drifting very far away from the $s_i$, as set by the hyperparameter
$v_\sigma$.


Putting it all together, we can write down the likelihood of the data, $D_i$, for the parameters $\theta$, on a single epoch, as
\begin{eqnarray}\displaystyle
p(D_i|\theta,I) &=& \int_0^{\infty} p(\fobs |\sigma_i,\theta,I)\, p(\sobs |\sigma_i,\theta,I)\, p(\sigma_i | \theta)\,\dd \sigma_i
\\
p(\sigma_i|\theta) &\propto& \sigma_i^{-1}
\\
\theta &\equiv& (\omega, \{A_k\}, \{B_k\}, s_\mu^2, B, V_B ,v_\sigma) \quad ,
\end{eqnarray}
where we have inserted the expressions from Equations (\ref{eq:mlik}) and (\ref{eq:slik}), and integrated out the nuisance parameter, $\sigma_i$.  We have assumed a Jeffrey's prior on $\sigma_i$, which is non-informative and invariant to reparametrization of the variance.  We could alternatively use an inverse-gamma prior, which takes two hyperparameters (and is conjugate in the case of a normal likelihood with unknown variance).  We have also assumed that $\fobs$ and $\sobs$ are conditionally independent given $\sigma_i$, $\theta$, and $I$.  This is a reasonable assumption in the case that a detection is made at epoch $t_i$.


Finally if we assume that the data collected at each epoch are independent given the model parameters, we have that
\begin{eqnarray}\displaystyle
p(D|\theta,I) &=& \prod_i p(D_i|\theta,I)
\quad.
\end{eqnarray}
This is the likelihood for the entire
data set (all the measurements and non-detections of this star from
all the epochs, as delivered by the survey) given the $2K + 5$ parameter vector $\theta$.  This model
``correctly'' or at least ``justifiably'' uses all of the information
available, without making strong assumptions about the survey or its
veracity.


\subsection{Detecting Variability}

We will describe here how we will do the likelihood ratio test of the model of constant flux to that of periodically varying flux.


\subsection{Mixture Model for $\fobs$ or $\sobs$}

This may be too much for this paper, but will be useful in the presence of crappy data.



\subsection{Implementation}
\label{ss:implemenation}



\joey{Describe Python implementation here}




\section{Experiments}
\label{sec:experiments}


\subsection{Simulating Faint Mira Variables}
\label{ss:mirasim}

In this first experiment, we begin with a well-observed Mira variable from the ASAS Catalog of Variable Stars (ACVS, \citealt{acvs}) , ASAS 235627-4947.2.  This star has a pulsation period of 266.6286 days and a Lomb-Scargle amplitude of 2.38 mag (V band).  Note there are no non-detections in the ASAS light curve.

To simulate faint Mira stars from ASAS 235627-4947.2, we use the following procedure:
\begin{enumerate}
\item Convert the observed magnitudes, $m_i$, and errors, $s_{m,i}$ to fluxes, $f_i$ and flux errors, $s_{f,i}$ (we assume a V-band zero-point of $3.67\times10^{-9}$ erg/s/cm$^2$/\AA)
\item For a given flux dimming parameter, $d$, sample the new fluxes, $\tilde{f}_i$, from a Gaussian distribution centered around $f_i / d$ with standard deviation sampled from the empirical ASAS distribution of $s_{f,i} | f_i$. \joey{I found that relationship to be linear with a slope $\approx 1$, which is probably BS since the ASAS mag errors are all crap.  So I added a reference value to each flux error to ensure a mag limit $\approx$ 14.5 mag.}
\item Fluxes that are not at least 5$\sigma$ above zero are denoted as non-detections and their flux estimates (and errors) are censored.
\end{enumerate}
Folded light curves of ASAS 235627-4947.2, dimmed by four different values of $d$, are plotted in Figure \ref{fig:mirasim}.


\begin{figure*}
\begin{center}
\includegraphics[angle=0,width=6.5in]{../plots/mira_simulated.pdf}
\end{center}
\caption{ Folded light curves for data simulated from the Mira variable ASAS 235627-4947.2 using the prescription in \S\ref{ss:mirasim}.  Blue tick marks along the bottom axis denote phases where there were non-detections.  As the original flux measurements are dimmed by higher factors, the troughs of the sinusoidal light curve are censored, resulting in an incomplete view of the data.  \label{fig:mirasim}}
\end{figure*}


\subsection{Results of Fitting Miras}


\section{Results: ASAS Light Curves}
\label{sec:results}

We use the methodology of \cite{2011arXiv1106.2832R} to select the top ASAS Mira  and RR Lyrae, Fundamental Mode candidates.  Using a posterior probability threshold of 0.8 gives us 1720 Mira  and 1029 RR Lyrae candidates.
 
 \subsection{Mira Variables}
 
 Show P - A relationship before and after using the method
 
 
 
 \subsection{RR Lyrae Variables}
 
  Show P - A relationship before and after using the method

There is an RRL P - A relationship (strong linear anti-correlation)
 

\section{Discussion}
\label{sec:discussion}

What people can do, starting from the data-taking procedure.

limitations



\acknowledgements


\bibliography{non_detect}

\end{document}
