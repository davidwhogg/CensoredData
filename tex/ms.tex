\documentclass[12pt,preprint]{aastex}

% Import the natbib package for bibliography inclusion. To get it to work right, first typeset the bibliography (BibTex), Command-Shift-B.
% Then, typeset the document with LaTeX, Command-Shift-L or just Command-T.
% To get proper reference citations using aastex (preprint manuscripts), first typeset as emulateapj with BibTex to create the BibTex
% files and then typeset again as aastex to get the manuscript with references.
\usepackage{natbib}
\usepackage{epsfig,graphicx,color}
\usepackage{longtable}
\usepackage{amsmath}

\bibliographystyle{apj}
% A few colors to replace the defaults for certain link types
\definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
\definecolor{darkorange}{rgb}{.71,0.21,0.01}
\definecolor{darkgreen}{rgb}{.12,.54,.11}

%-----------------------------------------------------------------------------
% The hyperref package gives us a pdf with properly built
% internal navigation ('pdf bookmarks' for the table of contents,
% internal cross-reference links, web links for URLs, etc.)
\usepackage{hyperref}
\hypersetup{pdftex,  % needed for pdflatex
  breaklinks=true,  % so long urls are correctly broken across lines
  colorlinks=true,
  urlcolor=blue,
  linkcolor=darkorange,
  citecolor=darkgreen,
  }

%Define this shorthand for scientific notation.
\newcommand{\expnt}[2]{\ensuremath{#1 \times 10^{#2}}}   % scientific notation
\newcommand{\x}{{\bf x}}
\newcommand{\X}{{\bf X}}
\newcommand{\y}{{\bf y}}
\newcommand{\z}{{\bf z}}
\newcommand{\new}{red}
\newcommand{\ptr}{p_{\rm tr}}
\newcommand{\pte}{p_{\rm te}}
\newcommand{\Ex}{\mathbf{E}}
\newcommand{\Prob}{\mathbf{P}}
\newcommand{\PrfL}{\widehat{P}_{\rm{RF,}\mathcal{L}}(y|\x)}
\newcommand{\PrfLx}{\widehat{P}_{\rm{RF,}\mathcal{L}\cup\x'}(y|\x)}
\newcommand{\TreeL}{\theta_{b, \mathcal{L}}(y|\x)}
\newcommand{\TreeLx}{\theta_{b, \mathcal{L}\cup\x'}(y|\x)}
 \def\hipp {{\it Hipparcos~}}


\newcommand{\dd}{\mathrm{d}}

\newcommand{\fobs}{f_i}
\newcommand{\sobs}{s^2_i}

\newcommand{\ftrue}{f_i^*}

% Commands for annotating the docs with fixme and inter-author notes.  See
% below for how to disable these.
%
% Define a \fixme command to mark visually things needing fixing in the draft,
% as well as similar commands for each author to leave initialed special
% comments in the document.
% For final printing or to simply disable these bright warnings, symlink
% (there's a target 'dist_on' in the makefile that does this) the file
% macros_state.tex to macros_off.tex

\newcommand{\fixme}[1] { \textcolor{red} {
{\fbox{ {\bf FIX}
\ensuremath{\blacktriangleright \blacktriangleright \blacktriangleright}}
{\bf #1}
\fbox{\ensuremath{\blacktriangleleft \blacktriangleleft \blacktriangleleft}
} } } }

% And similarly, one (less jarring, with fewer symbols and no boldface) command
% for each one of us to leave comments in the main text.
\newcommand{\james}[1] { \textcolor{blue} {
\ensuremath{\blacklozenge} {\bf james says:}  {#1}
\ensuremath{\blacklozenge} } }

\newcommand{\hogg}[1] { \textcolor{darkorange} {
\ensuremath{\blacksquare} {\bf hogg says:}  {#1}
\ensuremath{\blacksquare} } }

\newcommand{\dan}[1] { \textcolor{darkgreen} {
\ensuremath{\bigstar} {\bf dan says:}  {#1}
\ensuremath{\bigstar} } }

\newcommand{\joey}[1] { \textcolor{red} {
\ensuremath{\clubsuit} {\bf joey says:}  {#1}
\ensuremath{\clubsuit} } }%\documentclass[manuscript]{aastex}


\begin{document}
%%%%%%%% TITLE, AUTHORS, PUBLICATION STATUS, and ABSTRACT %%%%%%%%%

\shorttitle{Fitting the Undetected}
\shortauthors{Richards, Hogg, Long, Foreman-Mackey}
\title{Fitting the Undetected:  Light-curve Feature Estimation in the Presence of Non-detections}
\author{
Joseph W. Richards\altaffilmark{1, 2, 3, *},
David W. Hogg\altaffilmark{4, 5},
James P. Long\altaffilmark{2},
Daniel Foreman-Mackey\altaffilmark{4}
}
%
\altaffiltext{1}{Astronomy Department, University of California, Berkeley, CA, 94720-7450, USA}
\altaffiltext{2}{Statistics Department, University of California, Berkeley, CA, 94720-7450, USA}
\altaffiltext{3}{wise.io, 2150 Shattuck Ave., suite 525, Berkeley, CA, 94704, USA}
\altaffiltext{4}{Center for Cosmology and Particle Physics, Department of Physics, New York University, 4 Washington Place, New York, NY 10003, USA}
\altaffiltext{5}{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg, Germany}
\altaffiltext{*}{\texttt{jwrichar@stat.berkeley.edu}}

% Note the status (in preparation, MNRAS submitted date, ApJ accepted date, in press etc).
\slugcomment{DRAFT 2013-03-27 / in preparation}

\begin{abstract}
In multi-epoch imaging surveys, faint variable sources will not be
detectable at all epochs.  In principle, the best approach is to
perform photometric measurements (forced photometry) anyway.
%
In practice, most surveys only provide detection upper limits at some
epochs; in some not even upper limits but just the ``null''
information that the star was not detected.
%
Here we demonstrate with real data on periodic variables from the
\textsl{All-Sky Automated Survey} that these null observations can
in fact be very useful in fitting and parameter estimation.
%
We build a novel light-curve model that includes the per-epoch
detection thresholds as a set of latent parameters, one per data
point; the best-fit model is the maximum-likelihood model after
marginalizing out the latent thresholds.
%
The key assumption is that the detection thresholds are drawn from a
stationary distribution (the parameters of which are free to vary).
%
We demonstrate that this censored-data model outperforms traditional
fitting---in which null observations are discarded from the
fits---in estimating both periods and amplitudes for stars subject to
heavy censoring.
%
In particular, our model eradicates a failure mode in which
traditional fitting obtains an integer fraction of the correct period.
\end{abstract}

%\keywords{stars: variables: general -- methods: data analysis -- methods: statistical -- techniques: photometric}

\section{Introduction}
\label{sec:intro}

Telescopes don't make catalogs, they make images (Hogg \& Lang 20xx).
Nonetheless, some of the most important astronomical projects have produced enormously valuable catalogs
(the \textsl{SDSS}, for example CITATION).
There is a growing understanding that precise measurements of astronomical sources have to be made directly in the imaging data
(CITE Lang, Anderson, something in weak lensing):
It is impossible to pass the full information content of the imaging to the end user via a catalog
(which by necessity is built with point estimates of the quantities of interest).

The measurements of importance for variable-star science are relatively simple.
Unlike the measurements used for astrometry, extragalactic transients, and weak lensing,
where two-dimensional (morphological) information matters,
when variable stars are being observed, all the end user wants is accurate photometry of each source at each epoch.
One could hope that catalog-level information is sufficient:
the detailed image morphology is not required for lightcurve extraction, except when fields are crowded.
However, there are still limitations to working at the catalog level:
Catalog entries (nowadays) usually come with noise variance estimates,
but they do not typically give detailed information about the locations or influence of
bad detector pixels, cosmic rays, and flat-field variations;
all these things might be needed to construct a precise photometric noise model or to understand residuals or artifacts.
More importantly, catalogs are usually subject to hard \emph{censoring};
the catalogs (nowadays) only include measurements that are deemed (by the software or catalog makers)
to be \emph{significant detections}.

This could change.
In principle a catalog could contain a measurement at \emph{every} epoch for any source that is detected at \emph{any} epoch.
This kind of ``forced photometry'' would include many non-significant measurements,
but it would come closer to passing forward all the information in the imaging to users working at catalog level.
We recommend this approach to the catalog makers of the future (especially those constructing the \textsl{LSST} catalogs).

Less good than the comprehensive forced-photometry approach is to report, when measurements are not significant,
a well-defined ``upper limit'' for the brightness of each source.
This is rarely done, and upper limits are hard to use, but upper limits are an improvement over standard practice.
Standard practice is to \emph{report nothing at all} when a source is not detected significantly.
Indeed, it is often hard, when presented with a multi-epoch catalog,
to determine whether the reason a particular source is missing from the imaging at a particular epoch
is because the source was faint or because the source fell outside the imaging field of view!

In this contribution we look at improving catalog-level time-domain astronomy in the presence of \emph{censored data}.
The problem we consider is that of period and amplitude modeling for large-amplitude variable stars in a data set
in which \emph{no information is reported at all} at the epochs at which the star is not significantly detected.
Furthermore, we will imagine that we also have \emph{no information at all} about the sensitivity of the telescope
at the time of each non-detection; that is, we have no upper limits and we can't reconstruct them from any extant meta-data.
The non-detections are literally database ``null'' values.
This set-up may seem artificial, but it is in fact the situation we find ourselves in for various surveys,
including the All-Sky Automated Survey (\textsl{ASAS}, \citealt{2001ASPC..246...53P}), data from which we will model below.
What we will show is that \emph{the ``null'' non-detections are in fact very informative};
amplitude and period fitting will be wrong in general if the non-detections are ignored.

To make this more concrete, consider the photometric data from the \textsl{ASAS} survey,
depicted in \figurename~\ref{fig:badfit} for a single Mira variable.
The photometric detections are plotted and so are the epochs at which there are nulls (non-detections).
The standard fitting procedure, given censored data like these, is to \emph{ignore the null values}.
When the nulls are ignored, the period-fitting method finds a short period.
At that period, the model predicts the star to be bright at many phases in which the survey reported null observations.
However, the presence of a null observation means that it is likely that the source brightness has dipped below the detection limit at that epoch.
%That is, although we have no idea how or why the nulls were generated,
%they effectively provide strong evidence against the short period.
When we include the nulls in our model (as described in \S\ref{sec:model}), we find that the best fit is a longer-period, higher-amplitude model.
This longer period is obviously better because it models the star to be faint at epochs where nulls commonly occur.
This example clearly illustrates that the nulls are informative.
In this paper we demonstrate how to automatically incorporate that information in a rigorous statistical model.

The benefit of using the non-detections is large, but the cost is also large:
Use of these ``null'' data involves non-trivial modeling,
and large numbers of latent parameters to be inferred and marginalized out.
In particular it involves modeling the data-generating process,
which includes variable instrument and atmospheric conditions
(leading to different effective detection thresholds)
and also the software process by which catalog-entry decisions are made.
In practice our models will be very simplified, but it is impossible---in the presence of such noisy and imperfect data---to proceed \emph{without} significant modeling.

In what follows, we concentrate on parameter estimation---fitting of amplitudes and periods to noisy censored data%
---but because modeling, parameter estimation, discovery, and classification are intertwined,
this work has implications in all these areas.
For example, we often conclude that a star \emph{is} a periodic variable because parameter estimation delivers a solid period.
We often conclude that a star is a Mira or a RR Lyrae star because it's best-fit period or lightcurve shape is in some range.
For example, the valuable RR Lyrae catalog generated with \textsl{SDSS Stripe 82} data (CITE SESAR) classified stars
by performing parameter estimation and then cutting on derived parameters.
This study worked in the traditional mode of ignoring censored data, thereby limiting its ability to find the faintest
(and therefore most distant) RR Lyrae stars.
A model for the censored data might have extended the reach of this survey and led to new discoveries of Milky Way structure.
\hogg{Joey: add words here about your own variable-star catalog and what could be improved.}

\section{The Model}
\label{sec:model}

As a reminder, we are modeling light curve data in the presence of non-detections, which are epochs of observation in which no detection of the source of interest was made.

\subsection{Preliminaries}
\label{ss:prelim}

For each astronomical object, a photometric survey measures a multi-epoch light curve over $N$ (typically unevenly spaced) epochs.  At each epoch $i$, with associated time $t_i$, the survey takes an exposure at the location of the object and either (a) detects the object and records an estimate of its photon flux, $\fobs$ and the  variance of the statistical uncertainty in that estimate, $\sobs$, or (b) fails to detect the object.  In the latter case, most modern surveys either record a reference value to signify that no detection was made (as is done in ASAS) or an estimate of the upper detection limit, $b_i$, which is the minimum brightness that could have been detected.

There are many reasons that a source might not show up in a catalog.  These include:
\begin{itemize}
\item low S/N of the source, due either to a higher noise level or a fainter signal,
\item the source falling outside of the detection window (e.g., near chip edge),
\item occulting of the source by an artifact of the detector (e.g., hot pixels, masked out), or
\item the source was out-shone by an intervening object (e.g., asteroid, comet, variable star, airplane, etc.).
\end{itemize}
Here, we present a statistical model that can be used to detect variable sources and model their variability using multi-epoch light curves containing epochs of non-detection.  Previous efforts to detect and model variability using multi-epoch photometry have typically ignored non-detections (REFs) or used them in an ad-hoc manner lacking statistical rigor (REFs).  An exception is \citet{2009AJ....137.4400L}, who measure proper motions of sources in SDSS falling below the detection limit.


In this paper, we will assume that $\fobs$ and $\sobs$ take on real-valued numbers in epochs for which the source was detected and receive the reference value {\tt NA} in epochs where no detection was made. For notational convenience we assemble all the light curve data for one source into a data set $D$ given by
\begin{eqnarray}\displaystyle
D &\equiv& \{D_i\}_{i=1}^N
\\
D_i &\equiv& (\fobs, \sobs)
\quad ,
\end{eqnarray}
where $(\fobs, \sobs)$ are the light curve measurements  at $t_i$.  The goal, for each astronomical object, is to construct the likelihood for the data $D$, given a set of model parameters that describe the variability of the object and the characteristics of the observations, and then to maximize the likelihood with respect to the model parameters.% or to use it in further inference.

\subsection{Modeling Non-Detections}
To model the mean brightness of the light curve as a function of time, we use a multiple-harmonic Fourier model with angular oscillation frequency $\omega$,
\begin{eqnarray}\displaystyle
\mu_i' &=& A_0 + \sum_{k=1}^K A_k\, \sin (t_i \, \omega  k) + B_k\, \cos (t_i \, \omega  k)
\quad ,\label{eq:fourier}
\end{eqnarray}
where $\mu_i'$ is the model brightness estimate in magnitude at time $t_i$, $\sqrt{A_k^2 + B_k^2}$ is the amplitude of the $k^{\rm th}$ harmonic of the frequency $\omega$ and $\tan^{-1}(\frac{B_k}{A_k})$ is the relative phase offset of harmonic $k$.  The number of harmonics, $K$, can either be fixed or treated as a free parameter over which to optimize the likelihood.

We model the brightness in magnitude because periodic variation in magnitude is more sinusoidal than in flux. However since errors more approximately Gaussian in flux space, we convert the $\mu_i'$ to flux  using
\begin{equation*}
\mu_i = 10^6 \times 10^{-0.4 \mu_i'}
\end{equation*}
Now both the model predictions $\mu_i$ and the data $(f_i,s_i)$ are in flux.

In addition to the expected brightness in Equation \ref{eq:fourier}, we assume that there is uncertainty or inappropriateness in the model, leading to a \emph{fractional model variance} $\eta^2$ at each point (assumed constant but easily generalized) as well as random variation due to photon arrival counts, assumed normal mean 0 with variance $\sigma_i^2$. The reported uncertainties $s_i^2$ are an estimate of the $\sigma_i^2$, see Subsection \ref{sec:uncertainty}. This leads to a true flux measurement of
\begin{eqnarray}\displaystyle
\ftrue = \mu_i + e_i
\label{eq:truefluxdef}
\end{eqnarray}
where $e_i \sim N(0,\sigma_i^2 + \eta^2\,\mu_i^2)$. Recall that $f_i^*$ is a latent variable because we will not see it when an observation is censored. We instantiate a latent variable, $b_i$, to represent the detection limit, in units of flux, at epoch $i$.  The $b_i$ parameter is essential because it probabilistically constrains the possible values of the mean brightness, $\mu_i$, of the light curve when there is a non-detection.  By employing a hierarchical model for the distribution of $b_i$, we can fully utilize all of the information encoded in both the detections and non-detections when computing the data likelihood.   At each epoch, we connect the observed flux, $\fobs$, to the latent variable $\ftrue$ by
\begin{eqnarray}\displaystyle
\fobs &=& \left\{\begin{array}{ll}
  \ftrue & \mbox{if $\ftrue \ge b_i$} \\
  \texttt{NA} & \mbox{if $\ftrue < b_i$} \\
\end{array} \right.
\end{eqnarray}
The observed flux is \texttt{NA} only when the true flux, $\ftrue$, is below the detection limit $b_i$.

Using the model above, the likelihood of the observed $\fobs$ given $\sigma^2_i$, $\theta$ and $I$ (after integrating out the unknown $b_i$) is
\begin{eqnarray}\displaystyle
p(\fobs |\sigma^2_i,\theta,I) &=& \left\{\begin{array}{ll}
  N(\fobs | \mu_i,  \sigma^2_i + \eta^2\,\mu_i^2)\,  \int_{-\infty}^{\fobs} p(b_i | \theta)\, \dd b_i & \mbox{if $\fobs \ne \texttt{NA}$} \\
  \int_{-\infty}^{\infty} \int_{-\infty}^{b_i} N(\ftrue | \mu_i, \sigma^2_i + \eta^2\,\mu_i^2)\, p(b_i | \theta)\, \dd \ftrue\, \dd b_i & \mbox{if $\fobs = \texttt{NA}$} \\
\end{array}\right.\label{eq:mlik}
\\
p(b_i|\theta) &=& N(b_i|B,V_B)
\label{eq:bprior}
\\
\theta &\equiv& (\omega, A_0, \{A_k, B_k\}_{k=1}^K, \eta^2, B, V_B, \cdots) \quad ,
\end{eqnarray}
where we have introduced the hyperparameters $B$ and $V_B$ for the Gaussian prior distribution of $b_i$.  In Equation \ref{eq:mlik}, in the epochs for which a detection was made ($\fobs \ne \texttt{NA}$), we marginalize over the unknown $b_i$ up to the observed flux $\fobs$, enforcing that the detection limit be fainter than the observed brightness. Computationally, this requires a call to a Gaussian probability density function (pdf), a call to a Gaussian cumulative distribution function (cdf), and a multiplication. In the epochs for which no detection was made ($\fobs = \texttt{NA}$), we compute the probability that the unknown detection threshold $b_i$ is greater than the unknown true flux $f_i^*$. When both $b_i$ and $f_i^*$ are normally distributed, this can be computed using a single call to a Gaussian cdf.

In the above, we have assumed no extra information on each of the $b_i$ values besides the knowledge of whether a detection was made at that epoch.  Hence, we draw, in Equation \ref{eq:bprior}, each $b_i$ value from a global prior distribution which is the same at all epochs.  If instead, we are given an estimate of $b_i$ plus its error distribution for each epoch (which, in principle can be inferred from the raw telescope images), we can replace Equation \ref{eq:bprior} with a different distribution per epoch.  In the case that the $b_i$ are assumed to be completely known (without error), the data likelihood of $\fobs$ becomes
\begin{eqnarray}\displaystyle
p(\fobs |\sigma^2_i,\theta,I, b_i) &=& \left\{\begin{array}{ll}
  N(\fobs | \mu_i,  \sigma^2_i + \eta^2\,\mu_i^2)\,  I(\fobs \ge b_i) & \mbox{if $\fobs \ne \texttt{NA}$} \\
 \int_{-\infty}^{b_i} N(\ftrue | \mu_i, \sigma^2_i + \eta^2\,\mu_i^2)\, \dd \ftrue & \mbox{if $\fobs = \texttt{NA}$} \\
\end{array}\right.\label{eq:mlik_s}
\quad ,
\end{eqnarray}
where the boolean indictor function, $I(\fobs \ge b_i)$, is 1 if $\fobs \ge b_i$ and 0 otherwise.



\subsection{Modeling Uncertainty in Uncertainties}
\label{sec:uncertainty}
Instead of assuming that $\sobs$ is a perfect, error-free measurement, we can probabilistically connect it to the true photon flux uncertainty variance $\sigma^2_i$ through a Gamma likelihood.  Our model probability of observed $\sobs$ given $\fobs$, $\sigma^2_i$, $\theta$ and $I$, is
\begin{eqnarray}\displaystyle
p(\sobs | \fobs, \sigma^2_i, \theta,I) &=& \left\{\begin{array}{ll}
  \Gamma (\sobs | \sigma^2_i, V_{\sigma} ) & \mbox{if $\fobs \ne \texttt{NA}$} \\
 I(\sobs = \texttt{NA})& \mbox{if $\fobs = \texttt{NA}$}
\end{array}\right.\label{eq:slik}
\quad ,
\end{eqnarray}
where $\Gamma(x | m, V)$ is the standard Gamma distribution parameterized by its mean $m$ and variance $V$\footnote{The $\Gamma(x | m, V)$ pdf is $ p(x ) = (\theta^k \Gamma(k))^{-1}  x^{k-1} e^{x/ \theta}$, for $x \ge 0$, where $\theta = V / m$ and $k  = m^2 / V$.}.
The  model hyperparameter $V_\sigma$ represents the
variance in the distribution of reported uncertainties around the true
uncertainty.  Smaller values of $V_\sigma$ represent higher credence given to the reported uncertainties whereas larger values mean that our model is given more flexibility to use observation uncertainties that are far from the reported values.  Ultimately, the model will choose the value of $V_\sigma$, for each light curve, that maximizes the data likelihood.

To complete our model of uncertainties on the reported observational error variances, $s_i^2$, we must place a prior distribution on the true uncertainties, $\sigma_i^2$.  We choose to place a Gamma prior on the true photon flux uncertainties, 
\begin{eqnarray}\displaystyle
p(\sigma^2_i|\theta, I) &=& \Gamma(\sigma^2_i | S, V_S)
\label{eq:sigma_prior}
\end{eqnarray}
adding two parameters, $S$ and $V_S$, to our model.
\james{should probably justify choice of some priors here / include some plots and/or refs. gamma priors and normal priors.}  This puts us in a position to write down our full likelihood model for light curves containing censored observations.

\subsection{The Full Model}
\label{sec:full}
Putting it all together, we can write down the likelihood of the data, $D_i$, for the parameters $\theta$, on a single epoch, as
\begin{eqnarray}\displaystyle
\label{eqn:lik}
p(D_i|\theta,I) &=& \int_0^{\infty} p(\fobs |\sigma^2_i,\theta,I)\, p(\sobs | \fobs, \sigma^2_i,\theta,I)\, p(\sigma^2_i | \theta, I)\,\dd \sigma^2_i
\\
\label{eqn:likpar}
\theta &\equiv& (\omega, A_0, \{A_k, B_k\}_{k=1}^K, \eta^2, B, V_B, V_{\sigma}, S, V_S) \quad ,
\end{eqnarray}
where we have inserted the expressions from Equations (\ref{eq:mlik}), (\ref{eq:slik}), and (\ref{eq:sigma_prior}), and integrated over the nuisance parameter $\sigma^2_i$.

Finally if we assume that the data collected at each epoch are independent given the model parameters, we have that
\begin{eqnarray}\displaystyle
\label{eqn:likfull}
p(D|\theta,I) &=& \prod_i p(D_i|\theta,I)
\quad.
\end{eqnarray}
This is the likelihood for the entire
data set (all the measurements and non-detections of this star from
all the epochs, as delivered by the survey) given the $2K + 9$ parameter vector $\theta$.
Figure \ref{fig:graph} displays a graphical model highlighting dependencies and 
independence assumptions. This model
``correctly'' or at least ``justifiably'' uses all of the information
available, without making strong assumptions about the survey or its
veracity.

\subsection{Optimization in Practice}
\label{ss:optim}

Using the light curve data, $D$, from a particular source, our goal is to infer the parameters that describe the variability of that object.  We do so by maximizing the likelihood in Equations (\ref{eqn:lik}), and (\ref{eqn:likfull}) with respect to the parameter vector in Equation (\ref{eqn:likpar}).  The maximum likelihood estimate (MLE), which we call $\widehat{\theta}$ is the best estimate of the vector of model parameters that explain the variability of the object, given all of the data (observed and censored) and the assumed model.  The MLE allows us to then infer parameters of physical interest, such as the period and amplitude of oscillation.

To maximize the data likelihood with respect to $\theta$, we use the {\tt fmin} routine in the {\tt Python} module {\tt scipy.optimze}.  The {\tt fmin} routine uses a a Nelder-Mead simplex algorithm to optimize a function with respect to its arguments starting from some initial guess, $\theta_0$.  The convergence of the algorithm is highly sensitive to the value of $\theta_0$.  To find the MLE parameters for each light curve, we use {\tt fmin} to minimize the negative log likelihood (negative natural log of Equation \ref{eqn:likfull}).  We use a standard Lomb-Scargle Fourier analysis (ignoring any epochs of non-detection) to initialize the search algorithm, deriving an initial guess for $\omega$ as the frequency corresponding to the maximal Lomb-Scargle spectral density, and initializing $A_0$, and $\{A_k, B_k\}_{k=1}^K$ using standard least squares.  In practice, we run the {\tt fmin} routine multiple times using different initial values of the angular frequency corresponding to the frequencies of the most significant spectral density peaks and values of one-half those frequencies.  Initial values for the other parameters are set at $\eta^2 = 0.2$, $B = V_B = 2\cdot\min(\fobs)$, and $ V_{\sigma} = S = V_S = \textrm{med}(\sobs)$.



Replacing likelihood with fast version where we trust $s_i^2$ for observed data. \joey{I need to finish this}


\joey{we should include a link to the git repo}



\section{Experiment: Period Estimation for Mira Variables}
\label{sec:experiments}

In this section, we compare  the methodology from \S\ref{sec:model} to the traditional fitting method that does not incorporate null observations (the generalized Lomb-Scargle periodogram; \citealt{1976lomb, 1982scar, zech09}).  The goal in the simulation study is to compare the abilities of the two methods for period and amplitude estimation on a series of simulated Mira variables.  Of particular interest is to gauge the efficacy of each fitting method when applied to light curves at different observed (median) brightness levels.


\subsection{Simulating Faint Mira Variables}
\label{ss:mirasim}

In this first experiment, we begin with a well-observed Mira variable from the ASAS Catalog of Variable Stars (ACVS, \citealt{acvs}) , ASAS 235627-4947.2.  This star has a pulsation period of 266.5245 days and a Lomb-Scargle amplitude of 2.36 mag ($V$ band) based on 408 epochs of observations with zero null detections.  The median observed brightness of the ASAS light curve is $V = 10.4$.

We will employ the ASAS observations of this bright, well-observed star to simulate what the same star would look like at different median observed brightness levels.  As the star's observed brightness is artificially changed to fainter and fainter values, its simulated ASAS light curve will contain a larger and larger number of null observations.  Effectively, artificially dimming the star decreases the signal to noise ratio of each observation, which will cause some of the original detections to change into non-detections.

To simulate faint Mira stars from ASAS 235627-4947.2, we use the following procedure:
\begin{enumerate}
\item Convert the observed magnitudes $m_i$ and uncertainty variances $s^2_{m,i}$ to fluxes $f_i$ and flux uncertainty variances $s^2_{f,i}$ (we assume a $V$-band zero-point of $3.67\times10^{-9}$ erg/s/cm$^2$/\AA).
\item For a given flux dimming parameter, $d$, sample the new fluxes, $\tilde{f}_i$, from a Gaussian distribution centered around $f_i / d$ with standard deviation sampled from the empirical ASAS distribution of $s^2_{f,i} | f_i$. We simulate 10 different Mira variables by employing different values of $d$, ranging from 1 to 500.
\item Flux measurements of the `dimmed' light curve that are not at least 5$\sigma$ above zero are denoted as non-detections and their flux estimates (and errors) are censored and nulled out.
\end{enumerate}
Folded and unfolded versions of these simulated light curves are plotted in Figure \ref{fig:mirasim}.


\subsection{Results of Fitting Simulations}

We apply both the traditional method and our censored fitting method to each of the 10 simulated light curves.  Fits to four of the simulations are plotted in Figure \ref{fig:mirasim}.  In three of the four simulations, the traditional method estimates an incorrect period whereas our method recovers the true period, to within $\sim1 \%$, in each case.  Clearly, the censoring method works well because it incorporates the abundant null observations, effectively discovering the period which best places them  at phases for which the light-curve model dips below the inferred detection limit.  Remarkably, even for the faintest simulation ($V=16.7$), our method estimates the correct period even though only 7 epochs of observations were recorded (Figure \ref{fig:mirasim}d).  This demonstrates the value in utilizing the null detections: whereas the traditional method completely ignores the information in the 401 null non-detections, our model fully incorporates it when determining the best-fit period and sinusoid model.

Figure \ref{fig:mirasimparams} depicts the estimates of the period and amplitude of the Mira star as a function of the median $V$-band magnitude of the simulated source.  Whereas our method recovers the correct period for each simulation, the traditional period-finding method only works well for the five brightest simulations ($V \lesssim 14$).  For fainter magnitudes, the traditional method mistakenly detects a period of one-half or one-third the value of the true period.  The story for amplitude estimation is similar: even for the faintest stars, the estimates of our method are reasonably well-behaved (all within 1 mag of the true amplitude) whereas the amplitude estimates from the traditional method quickly fall to very small values as the brightness of the simulation decreases.

  It is important to note that our fitting method typically takes longer than the traditional generalized Lomb-Scargle periodogram to fit a light curve.  The amount of compute time for our method depends heavily on the number of null observations.  For example, on a single core of a MacBook Pro with 2.9 GHz Intel Core and 8 GB of memory, our method takes 291 seconds for the faintest simulation (401 null detections), 80 seconds for the simulation at 12.8th magnitude (105 null detections) and 0.24 seconds for the brightest simulation (0 null detections).  For comparison, the traditional method takes less than 0.1 seconds in each case.



\section{Application to Mira Light Curves in ASAS}
\label{sec:results}

We use the methodology of \cite{2011arXiv1106.2832R} to select the top ASAS Mira  and RR Lyrae, Fundamental Mode candidates.  Using a Mira classification probability threshold of 0.8 gives us 1720 Mira  and 1029 RR Lyrae candidates.

\joey{Note: ASAS provides periods and amplitude for all of the objects in ACVS!  We can and should use this as a comparison set}

\joey{From a brief reading of a few Mira papers, it seems that some papers throw away Miras for which no trough of the LC was observed.  This seems silly.  I have an idea of the P-A plots from Hipparcos.  Should be a good comparison set.  Also, we can consider applying this to Hipparcos, which has publicly available data.}


 \subsection{Mira Variables}

Figure \ref{fig:miraP} shows the estimated periods for standard Lomb-Scargle against the periods from our censored Fourier model for 2538 likely Mira variables chosen from the MACC catalog (these were chosen such that $P$(Mira) $>0.5$.)  For 12.6\% of these objects, the censored period is twice that of the Lomb-Scargle period, while for 0.51\% it is triple (show examples).  This occurs because of XYZ.  For 85.5\% of all objects, the periods are consistent to $\pm$5\%.



Figure \ref{fig:miraPA} shows the period-amplitude relationship for the set of Mira variables, with periods and amplitudes estimated by standard Lomb-Scargle (left) and the censored model (right).  Results show that XYZ.
 




\section{Discussion}
\label{sec:discussion}
% This section should NOT be a summary of the paper:  That lives in the abstract!

We have shown that even the apparently ``null'' information that a
source was not detected can nonetheless be very informative, if the
generation of the nulls is \emph{modeled}.  This is true even in the
worst-case scenario that telescope sensitivity is not known or
derivable from the data or any meta data.  The inclusion of the null
data can have important impacts on parameter estimation; in particular
we have shown that it improves variable-star period and amplitude
estimation in heavily censored data sets.

Modeling the nulls brings great benefit.  It comes at cost, however:
Optimizing the marginalized likelihood (marginalizing out the latent
parameters related to the censoring) is computationally expensive.
It would be even more expensive if we had permitted all the
variables in our graphical model (\figurename~\ref{fig:pgm}) to vary.
Our method is far slower than the traditional method of ignoring the
nulls and performing linear least-squares fitting.

There are also various limitations to our approach, or aspects of what
we have presented here that could use improvement.  One is that our
noise model and censoring model were both extremely naive.  We
effectively assumed that the censoring process was related very
directly to signal-to-noise; this might not be the case in many
experiments.  In particular, sources are often missed because of
detector issues or chip gaps or other hardware-related reasons.  These
nulls could be modeled by our model without modification, but it would
be a ``bad fit'' to the true data-generating process.  A better model
might be a ``mixture'' model that permits some censoring to be caused
by more arbitrary processes.

We also assumed that the censoring process is constant in time.  This
is not true in general; in most optical systems the censoring would
depend (at least in part) on the weather, and the weather depends on
season and year.  It would be straightforward to add parameters that
permit the censoring to depend on season and year.

\hogg{This section should return to the detection point and the promise of that.  Can we give an example in ASAS of a de novo detection?}


\acknowledgements It is a pleasure to thank
%
  Josh Bloom (Berkeley) and
  Nat Butler (ASU)
%
for valuable discussions and comments.
%
Partial support for this project was provided by \hogg{[JWR: PUT GRANT
  NUMBERS HERE]}, NASA grant NNX12AI50G, and NSF grant IIS-1124794.

\bibliography{non_detect}

\clearpage
\begin{figure}
  \begin{center}
\includegraphics[angle=0,width=6.5in]{fig_045832-0604_1.png}
  \end{center}
  \caption{A comparison of the best fit obtained by the traditional method (left) and our censored data model (right) on the Mira variable ASAS 045832-0604.1.  The top row shows the respective model fits to the raw light curves while the bottom row displays the period-folded data and models.  The black dots denote the observed data and the red vertical dashes mark the epochs of null observation.  The traditional fitting method clearly obtains half the true period while our method recovers the correct period.  The traditional method fails because it identifies a reasonable high-frequency fit to the observed data which completely ignores the null observations.  Note that alternate cycles of the traditional model fit contain no observed data.  By contrast, our method fully accounts for the null observations using a probability model.\label{fig:badfit}}
\end{figure}

\clearpage
\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth,%
      trim=3cm 12cm 5cm 3cm, clip=true]{gm/gm.pdf}
  \end{center}
  \caption{The graphical representation of the probability distribution.\label{fig:graph}}
\end{figure}




\clearpage
\begin{figure}
\begin{center}
$\begin{array}{c@{\hspace{-.3in}}c}
\multicolumn{1}{l}{\mbox{\bf (a)} \hspace{0.1in} V=16.7} & \multicolumn{1}{l}{\mbox{\bf (b)}\hspace{0.1in} V=15.5} \\      \vspace{-.1in}
\includegraphics[angle=0,width=3.5in]{fig_simulation_0_002_dat.png} & \includegraphics[angle=0,width=3.5in]{fig_simulation_0_007_dat.png} \\
\multicolumn{1}{l}{\mbox{\bf (c)}\hspace{0.1in} V=14.6} & \multicolumn{1}{l}{\mbox{\bf (d)}\hspace{0.1in} V=13.6} \\
\includegraphics[angle=0,width=3.5in]{fig_simulation_0_02_dat.png} & \includegraphics[angle=0,width=3.5in]{fig_simulation_0_05_dat.png}
\end{array}$
%\includegraphics[angle=0,width=6.5in]{../plots/mira_simulated.pdf}
\end{center}
\caption{ Results of period fitting analysis for 4 of the Mira simulations based on the ASAS 235627-4947.2 light curve.  In each figure, the left-hand column shows the result of the traditional fitting method and the right-hand column depicts the result of our method.  For each of the simulations shown, our method discovers the true period (to within $\sim 1\%$), even down to a median Mira magnitude of $V=16.7$ (panel a).  On the other hand, the traditional fitting method finds a wrong period for all but the brightest simulation plotted ($V=13.6$, panel d).  Likewise, the amplitude estimates for our method are clearly better than those of the traditional method.  Even with as few as 7 observed epochs of data, our method successfully recovers the true period because it utilizes the 400 null observations.    \label{fig:mirasim}}
\end{figure}


\clearpage
\begin{figure}
\begin{center}
\includegraphics[angle=0,width=6.5in]{simulation_parameter_estimates.png}
\end{center}
\caption{  Period and amplitude estimates of simulated Mira variables.  Top: Our method (dashed blue line) recovers the true period for each simulated brightness level while the traditional method fails for the five faintest simulations.  The dashed gray line denotes the `true' period, determined by analysis on the original, well-sampled Mira.  Bottom: For amplitude estimation, our method determines values which are much closer to the true value (dashed gray line), even for faint Mira simulations.  Because it ignores the null observations, the traditional method recovers  amplitudes that are much too small for faint Miras.    \label{fig:mirasimparams}}
\end{figure}



\clearpage
\begin{figure}
  \begin{center}
$\begin{array}{c@{\hspace{-.3in}}c}
\multicolumn{1}{l}{\mbox{\bf (a)} \hspace{0.7in} \textrm{ ASAS 034344+0655.5}} & \multicolumn{1}{l}{\mbox{\bf (b)}\hspace{0.7in}  \textrm{ ASAS 091617-2936.7}} \\      \vspace{-.1in}
\includegraphics[angle=0,width=3.5in]{fig_034344+0655_5.png} & \includegraphics[angle=0,width=3.5in]{fig_091617-2936_7.png} \\
\multicolumn{1}{l}{\mbox{\bf (c)}\hspace{0.7in}  \textrm{ ASAS 075855-1914.1}} & \multicolumn{1}{l}{\mbox{\bf (d)}\hspace{0.7in}  \textrm{ ASAS 075826-4019.8}} \\
\includegraphics[angle=0,width=3.5in]{fig_075855-1914_1.png} & \includegraphics[angle=0,width=3.5in]{fig_075826-4019_8.png}
\end{array}$
  \end{center}
  \caption{ \joey{add caption: period doubling} A real-data example.  This is the same source as that shown in \figurename~\ref{fig:badfit} but now showing the results of...\label{fig:goodfit}}
\end{figure}


\clearpage
\begin{figure}
  \begin{center}
$\begin{array}{c@{\hspace{-.3in}}c}
\multicolumn{1}{l}{\mbox{\bf (a)} \hspace{0.7in} \textrm{ ASAS 112811-6606.3}} & \multicolumn{1}{l}{\mbox{\bf (b)}\hspace{0.7in}  \textrm{ ASAS 065002-4554.6}} \\      \vspace{-.1in}
\includegraphics[angle=0,width=3.5in]{fig_112811-6606_3.png} & \includegraphics[angle=0,width=3.5in]{fig_065002-4554_6.png} \\
\multicolumn{1}{l}{\mbox{\bf (c)}\hspace{0.7in}  \textrm{ ASAS 092021-5815.2}} & \multicolumn{1}{l}{\mbox{\bf (d)}\hspace{0.7in}  \textrm{ ASAS 082012-3816.7}} \\
\includegraphics[angle=0,width=3.5in]{fig_092021-5815_2.png} & \includegraphics[angle=0,width=3.5in]{fig_082012-3816_7.png}
\end{array}$
  \end{center}
  \caption{\joey{add caption: period tripling} Another few real-data examples; in each case the method presented here either matches the traditional method or is substantially better.\label{fig:examples}}
\end{figure}



\clearpage
\begin{figure}
\begin{center}
\includegraphics[angle=0,width=6.5in]{../plots/mira_periods_LS_vs_Censored.pdf}
\end{center}
\caption{ .  \label{fig:miraP}}
\end{figure}

\clearpage
\begin{figure}
\begin{center}
\includegraphics[angle=0,width=6.5in]{../plots/mira_period_amplitude_relation.pdf}
\end{center}
\caption{ .  \label{fig:miraPA}}
\end{figure}



\end{document}
